@inproceedings{bolukbasi2016man,
    title={Man is to computer programmer as woman is to homemaker? {D}ebiasing word embeddings},
    author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
    booktitle={Advances in Neural Information Processing Systems},
    pages={4349--4357},
    year={2016}
}

@article{caliskan2017semantics,
    title={Semantics derived automatically from language corpora contain human-like biases},
    author={Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
    journal={Science},
    volume={356},
    number={6334},
    pages={183--186},
    year={2017},
    publisher={American Association for the Advancement of Science}
}

@inproceedings{hoyle-etal-2019-unsupervised,
    title = "Unsupervised Discovery of Gendered Language through Latent-Variable Modeling",
    author = "Hoyle, Alexander Miserlis  and
      Wolf-Sonkin, Lawrence  and
      Wallach, Hanna  and
      Augenstein, Isabelle  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1167",
    doi = "10.18653/v1/P19-1167",
    pages = "1706--1716",
    abstract = "Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.",
}

@inproceedings{zhao2018learning,
    title = "Learning Gender-Neutral Word Embeddings",
    author = "Zhao, Jieyu  and
      Zhou, Yichao  and
      Li, Zeyu  and
      Wang, Wei  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1521",
    doi = "10.18653/v1/D18-1521",
    pages = "4847--4853",
    abstract = "Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",
}

@inproceedings{knoche2019identifying,
author = {Knoche, Markus and Popovi\'{c}, Radomir and Lemmerich, Florian and Strohmaier, Markus},
title = {Identifying Biases in Politically Biased Wikis through Word Embeddings},
year = {2019},
isbn = {9781450368858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342220.3343658},
doi = {10.1145/3342220.3343658},
abstract = {With the increase of biased information available online, the importance of analysis and detection of such content has also significantly risen. In this paper, we aim to quantify different kinds of social biases using word embeddings. Towards this goal we train such embeddings on two politically biased MediaWiki instances, namely RationalWiki and Conservapedia. Additionally we included Wikipedia as an online encyclopedia, which is accepted by the general public. Utilizing and combining state-of-the-art word embedding models with WEAT and WEFAT, we display to what extent biases exist in the above-mentioned corpora. By comparing embeddings we observe interesting differences between different kinds of wikis.},
booktitle = {Proceedings of the 30th ACM Conference on Hypertext and Social Media},
pages = {253–257},
numpages = {5},
keywords = {conservapedia, rationalwiki, weat, stereotypes, embeddings, bias},
location = {Hof, Germany},
series = {HT '19}
}

@inproceedings{Fast2016EmpathUT,
    author = {Fast, Ethan and Chen, Binbin and Bernstein, Michael S.},
    title = {Empath: Understanding Topic Signals in Large-Scale Text},
    year = {2016},
    isbn = {9781450333627},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2858036.2858535},
    doi = {10.1145/2858036.2858535},
    abstract = {Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like "bleed" and "punch" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empath's data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC.},
    booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
    pages = {4647–4657},
    numpages = {11},
    keywords = {computational social science, fiction, social computing, NLP},
    location = {San Jose, California, USA},
    series = {CHI '16}
}

@inproceedings{manzini-etal-2019-black,
    title = "Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings",
    author = "Manzini, Thomas  and
      Yao Chong, Lim  and
      Black, Alan W  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1062",
    doi = "10.18653/v1/N19-1062",
    pages = "615--621",
    abstract = "Online texts - across genres, registers, domains, and styles - are riddled with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained on these texts, perpetuate and amplify these stereotypes, and propagate biases to machine learning models that use word embeddings as features. In this work, we propose a method to debias word embeddings in multiclass settings such as race and religion, extending the work of (Bolukbasi et al., 2016) from the binary setting, such as binary gender. Next, we propose a novel methodology for the evaluation of multiclass debiasing. We demonstrate that our multiclass debiasing is robust and maintains the efficacy in standard NLP tasks.",
}

@article{garg2018word,
    title={Word embeddings quantify 100 years of gender and ethnic stereotypes},
    author={Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
    journal={Proceedings of the National Academy of Sciences},
    volume={115},
    number={16},
    pages={E3635--E3644},
    year={2018},
    publisher={National Acad Sciences}
}


@article{kozlowski2019geometry,
    title={The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings},
    author={Kozlowski, Austin C and Taddy, Matt and Evans, James A},
    journal={American Sociological Review},
    volume={84},
    number={5},
    pages={905--949},
    year={2019},
    publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{gonen-goldberg-2019-lipstick-pig,
    title = "Lipstick on a Pig: {D}ebiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
    author = "Gonen, Hila  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1061",
    doi = "10.18653/v1/N19-1061",
    pages = "609--614",
    abstract = "Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between {``}gender-neutralized{''} words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",
}

@inproceedings{sweeney-najafian-2019-transparent,
    title = "A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings",
    author = "Sweeney, Chris  and
      Najafian, Maryam",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1162",
    doi = "10.18653/v1/P19-1162",
    pages = "1662--1667",
    abstract = "Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.",
}

@inproceedings{rudinger-etal-2017-social,
    title = "Social Bias in Elicited Natural Language Inferences",
    author = "Rudinger, Rachel  and
      May, Chandler  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the First {ACL} Workshop on Ethics in Natural Language Processing",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-1609",
    doi = "10.18653/v1/W17-1609",
    pages = "74--79",
    abstract = "We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The SNLI human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.",
}

@inproceedings{10.1145/2998181.2998187,
    author = {Joseph, Kenneth and Wei, Wei and Carley, Kathleen M.},
    title = {Girls Rule, Boys Drool: Extracting Semantic and Affective Stereotypes from Twitter},
    year = {2017},
    isbn = {9781450343350},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2998181.2998187},
    doi = {10.1145/2998181.2998187},
    abstract = {Social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. In the present work, we develop a method to extract the stereotypes of Twitter users. Our method is grounded in two distinct strands of theory, one that represents stereotypes as identities' affective meanings and the other that represents stereotypes as semantic relationships between identities. After validating our approach via a prediction task, we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies. Our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.},
    booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
    pages = {1362–1374},
    numpages = {13},
    keywords = {identity, computational social science, stereotype, social psychology, twitter},
    location = {Portland, Oregon, USA},
    series = {CSCW '17}
}

@inproceedings{kaneko-bollegala-2019-gender,
    title = "Gender-preserving Debiasing for Pre-trained Word Embeddings",
    author = "Kaneko, Masahiro  and
      Bollegala, Danushka",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1160",
    doi = "10.18653/v1/P19-1160",
    pages = "1641--1650",
    abstract = "Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender, racial or ethnic biases, which in turn bias the down-stream NLP applications that use those word embeddings. Taking gender-bias as a working example, we propose a debiasing method that preserves non-discriminative gender-related information, while removing stereotypical discriminative gender biases from pre-trained word embeddings. Specifically, we consider four types of information: \textit{feminine}, \textit{masculine}, \textit{gender-neutral} and \textit{stereotypical}, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information.",
}

@article{bhatia2018trait,
  title={Trait Associations for {H}illary {C}linton and {D}onald {T}rump in News Media: A Computational Analysis},
  author={Bhatia, Sudeep and Goodwin, Geoffrey P and Walasek, Lukasz},
  journal={Social Psychological and Personality Science},
  volume={9},
  number={2},
  pages={123--130},
  year={2018},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{bordia-bowman-2019-identifying,
    title = "Identifying and Reducing Gender Bias in Word-Level Language Models",
    author = "Bordia, Shikha  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-3002",
    doi = "10.18653/v1/N19-3002",
    pages = "7--15",
    abstract = "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN/Daily Mail{---}resulting in similar conclusions.",
}

@article{kumar-etal-2020-nurse,
    title = "Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings",
    author = "Kumar, Vaibhav  and
      Bhotia, Tenzin Singhay  and
      Kumar, Vaibhav  and
      Chakraborty, Tanmoy",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.32",
    doi = "10.1162/tacl_a_00327",
    pages = "486--503",
    abstract = "Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology that not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighboring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric, Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02{\%}. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).",
}

@inproceedings{park-etal-2018-reducing,
    title = "Reducing Gender Bias in Abusive Language Detection",
    author = "Park, Ji Ho  and
      Shin, Jamin  and
      Fung, Pascale",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1302",
    doi = "10.18653/v1/D18-1302",
    pages = "2799--2804",
    abstract = "Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, {``}You are a good woman{''} was considered {``}sexist{''} when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98{\%} and can be extended to correct model bias in other scenarios.",
}
